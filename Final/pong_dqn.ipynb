{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\james\\AppData\\Roaming\\Python\\Python39\\site-packages\\ale_py\\roms\\__init__.py:89: DeprecationWarning: Automatic importing of atari-py roms won't be supported in future releases of ale-py. Please migrate over to using `ale-import-roms` OR an ALE-supported ROM package. To make this warning disappear you can run `ale-import-roms --import-from-pkg atari_py.atari_roms`.For more information see: https://github.com/mgbellemare/Arcade-Learning-Environment#rom-management\n",
      "  ROMS = resolve_roms()\n"
     ]
    }
   ],
   "source": [
    "from lib import wrappers\n",
    "from lib import dqn_model\n",
    "import time\n",
    "import numpy as np\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\n",
    "MEAN_REWARD_BOUND = 19.5\n",
    "EPSILON_DECAY_LAST_FRAME = 10**5\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_FINAL = 0.02\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 32\n",
    "REPLAY_SIZE = 10000\n",
    "LEARNING_RATE = 1e-4\n",
    "SYNC_TARGET_FRAMES = 1000\n",
    "REPLAY_START_SIZE = 10000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experienced Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
    "\n",
    "class ExperienceBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
    "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
    "               np.array(dones, dtype=np.uint8), np.array(next_states)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, exp_buffer):\n",
    "        self.env = env\n",
    "        self.exp_buffer = exp_buffer\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        self.state = env.reset()\n",
    "        self.total_reward = 0.0\n",
    "\n",
    "    def play_step(self, net, epsilon=0.0, device=\"cpu\"):\n",
    "        done_reward = None\n",
    "\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            state_a = np.array([self.state], copy=False)\n",
    "            state_v = torch.tensor(state_a).to(device)\n",
    "            q_vals_v = net(state_v)\n",
    "            _, act_v = torch.max(q_vals_v, dim=1)\n",
    "            action = int(act_v.item())\n",
    "\n",
    "        # do step in the environment\n",
    "        new_state, reward, is_done, _ = self.env.step(action)\n",
    "        self.total_reward += reward\n",
    "\n",
    "        exp = Experience(self.state, action, reward, is_done, new_state)\n",
    "        self.exp_buffer.append(exp)\n",
    "        self.state = new_state\n",
    "        if is_done:\n",
    "            done_reward = self.total_reward\n",
    "            self._reset()\n",
    "        return done_reward"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(batch, net, tgt_net, device=\"cpu\"):\n",
    "    states, actions, rewards, dones, next_states = batch\n",
    "\n",
    "    states_v = torch.tensor(states).to(device)\n",
    "    next_states_v = torch.tensor(next_states).to(device)\n",
    "    actions_v = torch.tensor(actions).to(device)\n",
    "    rewards_v = torch.tensor(rewards).to(device)\n",
    "    done_mask = torch.ByteTensor(dones).to(device)\n",
    "\n",
    "    actions_v2 = actions_v.unsqueeze(-1).long()\n",
    "\n",
    "    state_action_values = net(states_v).gather(1, actions_v2).squeeze(-1)\n",
    "    \n",
    "    next_state_values = tgt_net(next_states_v).max(1)[0]\n",
    "    next_state_values[done_mask] = 0.0\n",
    "    next_state_values = next_state_values.detach()\n",
    "\n",
    "    expected_state_action_values = next_state_values * GAMMA + rewards_v\n",
    "    return nn.MSELoss()(state_action_values, expected_state_action_values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=6, bias=True)\n",
      "  )\n",
      ")\n",
      "1109 frames over 1 games, mean reward -19.000, eps 0.99, speed 709.92 f/s\n",
      "2087 frames over 2 games, mean reward -20.000, eps 0.98, speed 662.61 f/s\n",
      "3125 frames over 3 games, mean reward -20.000, eps 0.97, speed 594.22 f/s\n",
      "4212 frames over 4 games, mean reward -19.750, eps 0.96, speed 627.58 f/s\n",
      "5132 frames over 5 games, mean reward -19.800, eps 0.95, speed 567.53 f/s\n",
      "6015 frames over 6 games, mean reward -20.000, eps 0.94, speed 660.61 f/s\n",
      "6971 frames over 7 games, mean reward -20.000, eps 0.93, speed 535.78 f/s\n",
      "7762 frames over 8 games, mean reward -20.125, eps 0.92, speed 603.78 f/s\n",
      "8602 frames over 9 games, mean reward -20.111, eps 0.91, speed 587.41 f/s\n",
      "9788 frames over 10 games, mean reward -19.900, eps 0.90, speed 583.97 f/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\james\\AppData\\Local\\Temp\\ipykernel_28536\\98137925.py:15: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorAdvancedIndexing.cpp:1773.)\n",
      "  next_state_values[done_mask] = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10647 frames over 11 games, mean reward -19.909, eps 0.89, speed 30.84 f/s\n",
      "11428 frames over 12 games, mean reward -20.000, eps 0.89, speed 23.27 f/s\n",
      "12268 frames over 13 games, mean reward -20.000, eps 0.88, speed 21.69 f/s\n",
      "13031 frames over 14 games, mean reward -20.071, eps 0.87, speed 21.84 f/s\n",
      "13914 frames over 15 games, mean reward -20.133, eps 0.86, speed 21.70 f/s\n",
      "14677 frames over 16 games, mean reward -20.188, eps 0.85, speed 21.24 f/s\n",
      "15562 frames over 17 games, mean reward -20.235, eps 0.84, speed 21.41 f/s\n",
      "16398 frames over 18 games, mean reward -20.278, eps 0.84, speed 21.74 f/s\n",
      "17221 frames over 19 games, mean reward -20.316, eps 0.83, speed 21.75 f/s\n",
      "18159 frames over 20 games, mean reward -20.300, eps 0.82, speed 20.61 f/s\n",
      "19087 frames over 21 games, mean reward -20.286, eps 0.81, speed 20.46 f/s\n",
      "19906 frames over 22 games, mean reward -20.318, eps 0.80, speed 20.10 f/s\n",
      "20777 frames over 23 games, mean reward -20.304, eps 0.79, speed 20.33 f/s\n",
      "21755 frames over 24 games, mean reward -20.292, eps 0.78, speed 20.34 f/s\n",
      "22518 frames over 25 games, mean reward -20.320, eps 0.77, speed 20.24 f/s\n",
      "23281 frames over 26 games, mean reward -20.346, eps 0.77, speed 19.95 f/s\n",
      "24072 frames over 27 games, mean reward -20.370, eps 0.76, speed 19.76 f/s\n",
      "24912 frames over 28 games, mean reward -20.357, eps 0.75, speed 19.30 f/s\n",
      "25693 frames over 29 games, mean reward -20.379, eps 0.74, speed 25.36 f/s\n",
      "26547 frames over 30 games, mean reward -20.367, eps 0.73, speed 23.15 f/s\n",
      "27310 frames over 31 games, mean reward -20.387, eps 0.73, speed 22.41 f/s\n",
      "28193 frames over 32 games, mean reward -20.406, eps 0.72, speed 20.49 f/s\n",
      "29035 frames over 33 games, mean reward -20.424, eps 0.71, speed 21.12 f/s\n",
      "30170 frames over 34 games, mean reward -20.353, eps 0.70, speed 23.28 f/s\n",
      "31209 frames over 35 games, mean reward -20.343, eps 0.69, speed 23.34 f/s\n",
      "32120 frames over 36 games, mean reward -20.361, eps 0.68, speed 23.47 f/s\n",
      "33322 frames over 37 games, mean reward -20.351, eps 0.67, speed 23.46 f/s\n",
      "34280 frames over 38 games, mean reward -20.342, eps 0.66, speed 23.03 f/s\n",
      "35345 frames over 39 games, mean reward -20.308, eps 0.65, speed 22.99 f/s\n",
      "36394 frames over 40 games, mean reward -20.300, eps 0.64, speed 23.21 f/s\n",
      "37219 frames over 41 games, mean reward -20.317, eps 0.63, speed 22.48 f/s\n",
      "38102 frames over 42 games, mean reward -20.333, eps 0.62, speed 22.34 f/s\n",
      "38865 frames over 43 games, mean reward -20.349, eps 0.61, speed 21.48 f/s\n",
      "39748 frames over 44 games, mean reward -20.364, eps 0.60, speed 14.19 f/s\n",
      "40678 frames over 45 games, mean reward -20.356, eps 0.59, speed 21.47 f/s\n",
      "41604 frames over 46 games, mean reward -20.348, eps 0.58, speed 20.96 f/s\n",
      "42367 frames over 47 games, mean reward -20.362, eps 0.58, speed 20.60 f/s\n",
      "43158 frames over 48 games, mean reward -20.375, eps 0.57, speed 19.78 f/s\n",
      "44069 frames over 49 games, mean reward -20.388, eps 0.56, speed 21.35 f/s\n",
      "44920 frames over 50 games, mean reward -20.400, eps 0.55, speed 21.73 f/s\n",
      "45683 frames over 51 games, mean reward -20.412, eps 0.54, speed 21.28 f/s\n",
      "46534 frames over 52 games, mean reward -20.423, eps 0.53, speed 23.11 f/s\n",
      "47297 frames over 53 games, mean reward -20.434, eps 0.53, speed 23.77 f/s\n",
      "48060 frames over 54 games, mean reward -20.444, eps 0.52, speed 23.62 f/s\n",
      "48913 frames over 55 games, mean reward -20.455, eps 0.51, speed 21.97 f/s\n",
      "49753 frames over 56 games, mean reward -20.446, eps 0.50, speed 22.68 f/s\n",
      "50694 frames over 57 games, mean reward -20.456, eps 0.49, speed 21.70 f/s\n",
      "51485 frames over 58 games, mean reward -20.466, eps 0.49, speed 22.59 f/s\n",
      "52321 frames over 59 games, mean reward -20.458, eps 0.48, speed 20.28 f/s\n",
      "53102 frames over 60 games, mean reward -20.467, eps 0.47, speed 21.98 f/s\n",
      "53925 frames over 61 games, mean reward -20.475, eps 0.46, speed 23.13 f/s\n",
      "54716 frames over 62 games, mean reward -20.484, eps 0.45, speed 23.11 f/s\n",
      "55554 frames over 63 games, mean reward -20.492, eps 0.44, speed 22.59 f/s\n",
      "56317 frames over 64 games, mean reward -20.500, eps 0.44, speed 22.81 f/s\n",
      "57098 frames over 65 games, mean reward -20.508, eps 0.43, speed 23.54 f/s\n",
      "57949 frames over 66 games, mean reward -20.515, eps 0.42, speed 22.67 f/s\n",
      "58772 frames over 67 games, mean reward -20.522, eps 0.41, speed 22.67 f/s\n",
      "59563 frames over 68 games, mean reward -20.529, eps 0.40, speed 22.32 f/s\n",
      "60447 frames over 69 games, mean reward -20.536, eps 0.40, speed 23.24 f/s\n",
      "61238 frames over 70 games, mean reward -20.543, eps 0.39, speed 21.58 f/s\n",
      "62135 frames over 71 games, mean reward -20.535, eps 0.38, speed 22.29 f/s\n",
      "62959 frames over 72 games, mean reward -20.542, eps 0.37, speed 21.85 f/s\n",
      "63810 frames over 73 games, mean reward -20.548, eps 0.36, speed 20.77 f/s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m     epsilons\u001b[39m.\u001b[39mappend(epsilon)\n\u001b[0;32m     36\u001b[0m     frames\u001b[39m.\u001b[39mappend(frame_idx)\n\u001b[1;32m---> 38\u001b[0m reward \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mplay_step(net, epsilon, device\u001b[39m=\u001b[39;49mdevice)\n\u001b[0;32m     39\u001b[0m \u001b[39mif\u001b[39;00m reward \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     40\u001b[0m \u001b[39m# if reward is not None and reward > 0:\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     total_rewards\u001b[39m.\u001b[39mappend(reward)\n",
      "Cell \u001b[1;32mIn[4], line 24\u001b[0m, in \u001b[0;36mAgent.play_step\u001b[1;34m(self, net, epsilon, device)\u001b[0m\n\u001b[0;32m     21\u001b[0m     action \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(act_v\u001b[39m.\u001b[39mitem())\n\u001b[0;32m     23\u001b[0m \u001b[39m# do step in the environment\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m new_state, reward, is_done, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     25\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n\u001b[0;32m     27\u001b[0m exp \u001b[39m=\u001b[39m Experience(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, action, reward, is_done, new_state)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\gym\\core.py:314\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m--> 314\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m    315\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation(observation), reward, done, info\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\gym\\core.py:314\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m--> 314\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m    315\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation(observation), reward, done, info\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\gym\\core.py:314\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m--> 314\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m    315\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation(observation), reward, done, info\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\gym\\core.py:315\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m    314\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mstep(action)\n\u001b[1;32m--> 315\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobservation(observation), reward, done, info\n",
      "File \u001b[1;32mc:\\NEURO140\\Final\\lib\\wrappers.py:60\u001b[0m, in \u001b[0;36mProcessFrame84.observation\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mobservation\u001b[39m(\u001b[39mself\u001b[39m, obs):\n\u001b[1;32m---> 60\u001b[0m     \u001b[39mreturn\u001b[39;00m ProcessFrame84\u001b[39m.\u001b[39;49mprocess(obs)\n",
      "File \u001b[1;32mc:\\NEURO140\\Final\\lib\\wrappers.py:70\u001b[0m, in \u001b[0;36mProcessFrame84.process\u001b[1;34m(frame)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mUnknown resolution.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 70\u001b[0m img \u001b[39m=\u001b[39m img[:, :, \u001b[39m0\u001b[39;49m] \u001b[39m*\u001b[39;49m \u001b[39m0.299\u001b[39;49m \u001b[39m+\u001b[39;49m img[:, :, \u001b[39m1\u001b[39;49m] \u001b[39m*\u001b[39;49m \u001b[39m0.587\u001b[39;49m \u001b[39m+\u001b[39;49m img[:, :, \u001b[39m2\u001b[39;49m] \u001b[39m*\u001b[39;49m \u001b[39m0.114\u001b[39;49m\n\u001b[0;32m     71\u001b[0m resized_screen \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mresize(img, (\u001b[39m84\u001b[39m, \u001b[39m110\u001b[39m), interpolation\u001b[39m=\u001b[39mcv2\u001b[39m.\u001b[39mINTER_AREA)\n\u001b[0;32m     72\u001b[0m x_t \u001b[39m=\u001b[39m resized_screen[\u001b[39m18\u001b[39m:\u001b[39m102\u001b[39m, :]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# device = torch.device(\"gpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "env = wrappers.make_env(DEFAULT_ENV_NAME)\n",
    "\n",
    "net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "tgt_net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "writer = SummaryWriter(comment=\"-\" + DEFAULT_ENV_NAME)\n",
    "print(net)\n",
    "\n",
    "buffer = ExperienceBuffer(REPLAY_SIZE)\n",
    "agent = Agent(env, buffer)\n",
    "epsilon = EPSILON_START\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "total_rewards = []\n",
    "frame_idx = 0\n",
    "ts_frame = 0\n",
    "ts = time.time()\n",
    "best_mean_reward = None\n",
    "\n",
    "NUM_GAMES = 350\n",
    "\n",
    "# for plots\n",
    "mean_rewards = []\n",
    "num_games = []\n",
    "epsilons = []\n",
    "frames = []\n",
    "\n",
    "\n",
    "while len(total_rewards) < NUM_GAMES:\n",
    "    frame_idx += 1\n",
    "    epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME)\n",
    "    if frame_idx % 100 == 0:\n",
    "        epsilons.append(epsilon)\n",
    "        frames.append(frame_idx)\n",
    "\n",
    "    reward = agent.play_step(net, epsilon, device=device)\n",
    "    if reward is not None:\n",
    "    # if reward is not None and reward > 0:\n",
    "        total_rewards.append(reward)\n",
    "        speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
    "        ts_frame = frame_idx\n",
    "        ts = time.time()\n",
    "        mean_reward = np.mean(total_rewards[-100:])\n",
    "        mean_rewards.append(mean_reward)\n",
    "        num_games.append(len(total_rewards))\n",
    "        print(\"%d frames over %d games, mean reward %.3f, eps %.2f, speed %.2f f/s\" % (\n",
    "            frame_idx, len(total_rewards), mean_reward, epsilon,\n",
    "            speed\n",
    "        ))\n",
    "        writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
    "        writer.add_scalar(\"speed\", speed, frame_idx)\n",
    "        writer.add_scalar(\"reward_100\", mean_reward, frame_idx)\n",
    "        writer.add_scalar(\"reward\", reward, frame_idx)\n",
    "        if best_mean_reward is None or best_mean_reward < mean_reward:\n",
    "            torch.save(net.state_dict(), DEFAULT_ENV_NAME + \"-best.dat\")\n",
    "            if best_mean_reward is not None:\n",
    "                print(\"Best mean reward updated %.3f -> %.3f, model saved\" % (best_mean_reward, mean_reward))\n",
    "            best_mean_reward = mean_reward\n",
    "        if mean_reward > MEAN_REWARD_BOUND:\n",
    "            print(\"Solved in %d frames!\" % frame_idx)\n",
    "            break\n",
    "\n",
    "    if len(buffer) < REPLAY_START_SIZE:\n",
    "        continue\n",
    "\n",
    "    if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
    "        tgt_net.load_state_dict(net.state_dict())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    batch = buffer.sample(BATCH_SIZE)\n",
    "    loss_t = calc_loss(batch, net, tgt_net, device='cpu')\n",
    "    loss_t.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "writer.close()\n",
    "\n",
    "plt.plot(num_games, mean_rewards)\n",
    "plt.xlabel('Number of games played')\n",
    "plt.ylabel('Mean reward')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(1, len(total_rewards)+1), total_rewards)\n",
    "plt.xlabel('Number of games played')\n",
    "plt.ylabel('Total reward')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(total_rewards, bins=30)\n",
    "plt.xlabel('Reward')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(frames, epsilons)\n",
    "plt.xlabel('Number of frames played')\n",
    "plt.ylabel('Epsilon')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.11.0\n",
      "Keras Version: 2.11.0\n",
      "GPU is NOT AVAILABLE\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tensorflow version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {tf.keras.__version__}\")\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
