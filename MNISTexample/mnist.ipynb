{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0224, -0.2315,  0.9420,  0.3910, -2.6430], device='cuda:0')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(5).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(28 * 28, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.1),\n",
    "    nn.Linear(64, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fanicer Model\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(28 * 28, 64)\n",
    "        self.l2 = nn.Linear(64, 64)\n",
    "        self.l3 = nn.Linear(64, 10)\n",
    "        self.do = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h1 = nn.functional.relu(self.l1(x))\n",
    "        h2 = nn.functional.relu(self.l2(h1))\n",
    "        do = self.do(h2 + h1)\n",
    "        logits = self.l3(do)\n",
    "        return logits\n",
    "model = ResNet().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimiser = optim.SGD(model.parameters(), lr = 1e-2)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, Val split\n",
    "train_data = datasets.MNIST('data', train=True, download=True, transform=transforms.ToTensor())\n",
    "train, val = random_split(train_data, [55000, 5000])\n",
    "train_loader = DataLoader(train, batch_size =32)\n",
    "val_loader = DataLoader(val, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, train loss: 0.86, train acc: 0.78\n",
      "Epoch 1, validation loss: 0.381, train acc: 0.888\n",
      "Epoch 2, train loss: 0.38, train acc: 0.89\n",
      "Epoch 2, validation loss: 0.300, train acc: 0.905\n",
      "Epoch 3, train loss: 0.31, train acc: 0.91\n",
      "Epoch 3, validation loss: 0.259, train acc: 0.917\n",
      "Epoch 4, train loss: 0.27, train acc: 0.92\n",
      "Epoch 4, validation loss: 0.228, train acc: 0.929\n",
      "Epoch 5, train loss: 0.24, train acc: 0.93\n",
      "Epoch 5, validation loss: 0.206, train acc: 0.936\n"
     ]
    }
   ],
   "source": [
    "# Training and validation loops\n",
    "nb_epochs = 5\n",
    "for epoch in range(nb_epochs):\n",
    "    losses = list()\n",
    "    accuracies = list()\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        x, y = batch\n",
    "\n",
    "        # x: b x 1 x 28 x 28\n",
    "        b = x.size(0)\n",
    "        x = x.view(b, -1).cuda()\n",
    "\n",
    "        #1 forward training\n",
    "        l = model(x)  # l: logits\n",
    "\n",
    "        #2 obective function\n",
    "        J = loss(l, y.cuda())\n",
    "\n",
    "        #3 cleaning the gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        #4 accumulate the partial derivatives of J w.r.t. the parameters\n",
    "        J.backward()\n",
    "\n",
    "        #5 step in opposite direction of the gradient\n",
    "        optimiser.step()\n",
    "            #with torch.no_grad(): params = params - eta * params.grad (manual version of step above) (params is dict so use for every item in...)\n",
    "\n",
    "        losses.append(J.item())\n",
    "        accuracies.append(y.eq(l.detach().argmax(dim=1).cpu()).float().mean())\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, train loss: {torch.tensor(losses).mean():.2f}, train acc: {torch.tensor(accuracies).mean():.2f}')\n",
    "\n",
    "    losses = list()\n",
    "    accuracies = list()\n",
    "    model.eval()\n",
    "\n",
    "    for batch in val_loader:\n",
    "        x, y = batch\n",
    "\n",
    "        # x: b x 1 x 28 x 28\n",
    "        b = x.size(0)\n",
    "        x = x.view(b, -1).cuda()\n",
    "\n",
    "        #1 forward training\n",
    "        with torch.no_grad():\n",
    "            l = model(x) # logits\n",
    "\n",
    "        #2 computing the obective function\n",
    "        J = loss(l, y.cuda())\n",
    "\n",
    "        losses.append(J.item())\n",
    "        accuracies.append(y.eq(l.detach().argmax(dim=1).cpu()).float().mean())\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, validation loss: {torch.tensor(losses).mean():.3f}, train acc: {torch.tensor(accuracies).mean():.3f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9550a44aef1ca025e754d258d2389ec3d5189e78405733c3c64ef2c4fedbed91"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
